{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LinearRegression from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현할 것\n",
    "- 공부시간과 성적간의 관계를 모델링한다.\n",
    "    - **머신러닝 모델(모형)이란** 수집한 데이터를 기반으로 입력값(Feature)와 출력값(Target)간의 관계를 하나의 공식으로 정의한 함수이다. 그 공식을 찾는 과정을 **모델링**이라고 한다.\n",
    "    - 이 예제에서는 공부한 시간으로 점수를 예측하는 모델을 정의한다.\n",
    "    - 입력값과 출력값 간의 관계를 정의할 수있는 다양한 함수(공식)이 있다. 여기에서는 딥러닝과 관계가 있는 **Linear Regression** 을 사용해본다.\n",
    "\n",
    "# 데이터 확인\n",
    "- 입력데이터: 공부시간\n",
    "- 출력데이터: 성적\n",
    "\n",
    "|공부시간|점수|\n",
    "|-|-|\n",
    "|1|20|\n",
    "|2|40|\n",
    "|3|60|\n",
    "\n",
    "우리가 수집한 공부시간과 점수 데이터를 바탕으로 둘 간의 관계를 식으로 정의 할 수 있으면 **내가 몇시간 공부하면 점수를 얼마 받을 수 있는지 예측할 수 있게 된다.**   \n",
    "수집한 데이터를 기반으로 앞으로 예측할 수있는 모형을 만드는 것이 머신러닝 모델링이다.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습(훈련) 데이터셋 만들기\n",
    "- 모델을 학습시키기 위한 데이터셋을 구성한다.\n",
    "- 입력데이터와 출력데이터을 따로 행렬로 구성한다.\n",
    "- 같은 데이터 포인트의 입력, 출력 데이터를 같은 index에 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링\n",
    "\n",
    "## 모델 정의\n",
    "\n",
    "- Feature와 Target간의 관계를 수식으로 정의한다.\n",
    "- 여기서는 공부시간(Feature)와 점수(Target)간의 관계를 정의하는데 **선형회귀(Linear Regression) 모델** 을 가설로 세우고 모델링을 한다.\n",
    "    - 많은 머신러닝 연구자들이 다양한 종류의 데이터에 관계를 예측할 수 있는 여러 알고리즘을 연구했다.\n",
    "    - 선형회귀 모델은 입력데이터와 출력데이터가 선형관계(비례 또는 반비례 관계)일때 좋은 성능을 나타낸다.\n",
    "  \n",
    "> ### 가설\n",
    "> - 아직은 이 식이 맞는지 틀린지는 알 수없기 때문에 **이 식을 가설(hypothesis) 라고 한다.**\n",
    "> - 가설을 세우고 모델링을 한 뒤 검증을 해서 좋은 예측결과를 내면 그 가설을 최종 결과 모델로 결정한다. 예측결과가 좋지 않을 경우 새로운 가설로 모델링을 한다.\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀 (Linear Regression)\n",
    "- Feature들의 가중합을 이용해 Target을 추정한다.\n",
    "- Feature에 곱해지는 가중치(weight)들은 각 Feature가 Target 얼마나 영향을 주는지 영향도가 된다.\n",
    "    - 음수일 경우는 target값을 줄이고 양수일 경우는 target값을 늘린다.\n",
    "    - 가중치가 0에 가까울 수록 target에 영향을 주지 않는 feature이고 0에서 멀수록 target에 많은 영향을 주는 target이 된다.\n",
    "- 모델 학습과정에서 가장 적절한 Feature의 가중치를 찾아야 한다.\n",
    "      \n",
    " \n",
    "$$\n",
    "\\hat{y} = Wx + b\n",
    "$$\n",
    "<center>$\\hat{y}$: 모델추정값<br>W: 가중치<br>x: Feature<br>b: bias(편향)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [경사하강법을 이용한 최적화](03_2_gradient_decending.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 훈련(train) 데이터 정의\n",
    "# 입력데이터 변수명: X, 출력데이터 변수명: y\n",
    "X_train = torch.tensor([[1],\n",
    "                        [2],\n",
    "                        [3]\n",
    "                       ], dtype=torch.float32)\n",
    "y_train = torch.tensor([[20],\n",
    "                        [40],\n",
    "                        [60]\n",
    "                       ], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1]) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "# [3, 1] -> [데이터개수, 개별데이터의 shape] => 3개의 데이터, 각 데이터는 1개의 값으로 구성된 1차원배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 파라미터\n",
      "weight: tensor([[1.5410]], requires_grad=True)\n",
      "bias tensor([-0.2934], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "# weight와 bias를 정의\n",
    "weight = torch.randn(1, 1, requires_grad=True)  # (1: 입력 값의 개수, 1: 출력 값의 개수)\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "print('초기 파라미터')\n",
    "print('weight:', weight)\n",
    "print('bias', bias)\n",
    "\n",
    "def linear_model(X):\n",
    "    pred = X @ weight + bias\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2476],\n",
       "        [2.7886],\n",
       "        [4.3296]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 예측\n",
    "pred_train = linear_model(X_train)\n",
    "pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 오차함수 정의 -> 모델이 추론한 값과 정답사이의 차이를 계산하는 함수\n",
    "# 평균 제곱 오차 (Mean Squared Error: MSE)\n",
    "def mse_loss_fn(pred: '예측깂', y:'정답'):\n",
    "    return torch.mean((pred-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오차: tensor(1611.8477, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = mse_loss_fn(pred_train, y_train)\n",
    "print('오차:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20.],\n",
       "        [40.],\n",
       "        [60.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weight와 bias에 대한 gradient 계산\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight의 grad: tensor([[-173.4578]])\n",
      "bias의 grad: tensor([-74.4229])\n"
     ]
    }
   ],
   "source": [
    "print('weight의 grad:', weight.grad)\n",
    "print('bias의 grad:', bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update전 weight: tensor([[1.7145]], requires_grad=True)\n",
      "update후 weight: tensor([[1.8879]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 최적화 -> 경사하강법 ==> 파이토치의 함수를 사용\n",
    "optimizer = torch.optim.SGD([weight, bias], # 최적화할 대상 => requires_grad=True\n",
    "                            lr = 0.001  # 학습률\n",
    "                           )\n",
    "# 경사하강법 연산\n",
    "print('update전 weight:', weight)\n",
    "optimizer.step()\n",
    "print('update후 weight:', weight)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이전: tensor(1611.8477, grad_fn=<MeanBackward0>)\n",
      "업데이트 후: tensor(1576.4189, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## update 후 loss 계산\n",
    "# 추론\n",
    "pred = linear_model(X_train)\n",
    "# 오차 계산\n",
    "loss2 = mse_loss_fn(pred, y_train)\n",
    "print('이전:', loss)\n",
    "print('업데이트 후:', loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n",
    "1. 모델을 이용해 추정한다.\n",
    "   - pred = model(input)\n",
    "1. loss를 계산한다.\n",
    "   - loss = loss_fn(pred, target)\n",
    "1. 계산된 loss를 파라미터에 대해 미분하여 계산한 gradient 값을 각 파라미터에 저장한다.\n",
    "   - loss.backward()\n",
    "1. optimizer를 이용해 파라미터를 update한다.\n",
    "   - optimizer.step()  \n",
    "1. 파라미터의 gradient(미분값)을 0으로 초기화한다.\n",
    "   - optimizer.zero_grad()\n",
    "- 위의 단계를 반복한다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STEPS = 100  # 파라미터(WEIGHT, BIAS)를 업데이트할 횟수\n",
    "for _ in range(STEPS):\n",
    "    # 1. 추론(예측)\n",
    "    pred = linear_model(X_train)\n",
    "    # 2. 오차 계산\n",
    "    loss = mse_loss_fn(pred, y_train)\n",
    "    # 3. weight, bias에 대한 gradient를 계산\n",
    "    loss.backward()\n",
    "    # 4. 최적화 -> optimizer를 이용 ==> 경사하강법\n",
    "    optimizer.step()\n",
    "    # 5. 계산된 gradient값을 초기화\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: tensor([[17.2053]], requires_grad=True)\n",
      "bias: tensor([5.8763], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## weight, bias 값\n",
    "print('weight:', weight)\n",
    "print('bias:', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 오차 계산\n",
    "pred3 = linear_model(X_train)\n",
    "loss3 = mse_loss_fn(pred3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2893, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2998, grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss3**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 입력, 다중 출력\n",
    "- 다중입력: Feature가 여러개인 경우\n",
    "- 다중출력: Output 결과가 여러개인 경우\n",
    "\n",
    "다음 가상 데이터를 이용해 사과와 오렌지 수확량을 예측하는 선형회귀 모델을 정의한다.  \n",
    "[참조](https://www.kaggle.com/code/aakashns/pytorch-basics-linear-regression-from-scratch)\n",
    "\n",
    "\n",
    "|온도(F)|강수량(mm)|습도(%)|사과생산량(ton)|오렌지생산량|\n",
    "|-|-|-|-:|-:|\n",
    "|73|67|43|56|70|\n",
    "|91|88|64|81|101|\n",
    "|87|134|58|119|133|\n",
    "|102|43|37|22|37|\n",
    "|69|96|70|103|119|\n",
    "\n",
    "```\n",
    "사과수확량  = w11 * 온도 + w12 * 강수량 + w13 * 습도 + b1\n",
    "오렌지수확량 = w21 * 온도 + w22 * 강수량 + w23 *습도 + b2\n",
    "```\n",
    "\n",
    "- `온도`, `강수량`, `습도` 값이 사과와, 오렌지 수확량에 어느정도 영향을 주는지 가중치를 찾는다.\n",
    "    - 모델은 사과의 수확량, 오렌지의 수확량 **두개의 예측결과를 출력**해야 한다.\n",
    "    - 사과에 대해 예측하기 위한 weight 3개와 오렌지에 대해 예측하기 위한 weight 3개 이렇게 두 묶음, 총 6개의 weight를 정의하고 학습을 통해 가장 적당한 값을 찾는다.\n",
    "        - 이 묶음을 딥러닝에서는 **Node, Unit, Neuron** 이라고 한다.\n",
    "- 목적은 우리가 수집한 train 데이터셋을 이용해 **정확한 예측을 위한 weight와 bias**를 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "- Train data는 feature와 target를 각각 따로 2개의 행렬로 구성한다.\n",
    "- Feature의 행은 관측치(개별 데이터)를 열을 Feature(특성, 변수)를 표현한다.\n",
    "- Target은 모델이 예측할 대상으로 행은 개별 관측치, 열은 각 항목에 대한 정답으로 이 예제에서는 사과수확량과 오렌지 수확량 값을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity) : (5, 3)\n",
    "inputs = torch.tensor([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Targets: 생산량 - (apples, oranges) - (5, 2)\n",
    "targets = torch.tensor([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model (from scratch)\n",
    "\n",
    "### weight와 bias\n",
    "- weight 는 각 feature에 곱해지는 가중치로 target 값에 얼마나 영향을 주는지를 나타낸다. 직선의 방정식에서 기울기이다.\n",
    "- bias는 각 feature와 weight간의 가중합에 더해주는 값으로 모든 feature가 0일 경우 target의 값을 나타낸다. 직선의 방정식에서 절편이다.\n",
    "- weight와 bias는 각각 random 값을 초기값으로 가지는 matrix로 정의한다.\n",
    "- weight의 shape: (2, 3)\n",
    "- bias의 shape: (2, ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb1b8e427f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight와 bias를 생성\n",
    "# 파라미터 -> 학습을 통해 찾아야 하는 값 -> requires_grad=True\n",
    "weights = torch.randn(3, 2, requires_grad=True)  \n",
    "# [3, 2] => [input feature 개수, output feature 개수] => [[온도w, 강수량w, 습도w], [사과, 오렌지]]\n",
    "bias = torch.randn(2, requires_grad=True)  # [2] => [output 개수=>사과, 오렌지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([2])\n",
      "tensor([[ 1.5410, -0.2934],\n",
      "        [-2.1788,  0.5684],\n",
      "        [-1.0845, -1.3986]], requires_grad=True)\n",
      "tensor([0.4033, 0.8380], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(weights.shape, bias.shape)\n",
    "print(weights)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression model\n",
    "모델은 weights `w`와 inputs `x`의 내적(dot product)한 값에 bias `b`를 더하는 간단한 함수이다. \n",
    "\n",
    "$$\n",
    "\\hspace{2.5cm} X \\hspace{1.1cm} \\cdot \\hspace{1.2cm} W^T \\hspace{1.2cm}  + \\hspace{1cm} b \\hspace{2cm}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{cc}\n",
    "73 & 67 & 43 \\\\\n",
    "91 & 88 & 64 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "69 & 96 & 70\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\cdot\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "w_{11} & w_{21} \\\\\n",
    "w_{12} & w_{22} \\\\\n",
    "w_{13} & w_{23}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "+\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "b_{1} & b_{2} \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X):\n",
    "    return X @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.shape\n",
    "pred = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -79.7173,  -42.6370],\n",
       "        [-120.5089,  -65.3522],\n",
       "        [-220.3900,  -29.6390],\n",
       "        [  23.7697,  -56.3972],\n",
       "        [-178.3483,  -62.7408]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pred.shape)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "모델이 예측한 값과 정답간의 차이를 비교하는 메소드. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_fn(predㄴ, targets):\n",
    "    squared_error = (pred - targets) ** 2\n",
    "    return torch.mean(squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse_loss_fn(pred, targets)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36193.4961, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients 계산\n",
    "loss에 대한 weight와 bias의 gradients (미분계수)를 계산한다. **Pytorch의 자동미분**을 이용한다. (**graident를 구하려는 tensor는 requires_grad=True로 설정한다.**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5410, -0.2934],\n",
      "        [-2.1788,  0.5684],\n",
      "        [-1.0845, -1.3986]], requires_grad=True)\n",
      "tensor([[-15400.8262, -11915.3555],\n",
      "        [-19847.4922, -13088.5000],\n",
      "        [-11609.1875,  -8220.1094]])\n"
     ]
    }
   ],
   "source": [
    "print(weights)\n",
    "print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.9418, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weight_0_0 = weights[0, 0] - weights.grad[0, 0] * 0.001#lr\n",
    "new_weight_0_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4033, 0.8380], requires_grad=True)\n",
      "tensor([-191.2390, -143.3532])\n"
     ]
    }
   ],
   "source": [
    "print(bias)\n",
    "print(bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 최적화\n",
    "gradient decent 알고리즘을 이용해 loss를 줄여 모델의 추론 성능을 높인다. 이를 위해 좋은 성능을 낼 수 있도록 **경사하강법(gradient decent)** 을 이용해 weight와 bias를 update한다. \n",
    "\n",
    "1. 추론하기\n",
    "2. loss 계산하기\n",
    "3. weight와 bias에 대한 gradient계산하기\n",
    "4. 계산된 gradient에 비례한 값을 학습률을 곱해 작게 만든 뒤 wegith에서 빼서 조정한다.\n",
    "5. gradient를 0으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step(파라미터 업데이트)수 지정\n",
    "STEPS = 100\n",
    "# 학숩률 (Learning Rate)\n",
    "LR = 0.0001  # 1e-4\n",
    "# Optimizer 생성\n",
    "optimizer = torch.optim.SGD([weights, bias], lr=LR)  # 경사하강법처리. gradient값 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.8659443855285645\n",
      "loss: 4.954817771911621\n",
      "loss: 4.199535846710205\n",
      "loss: 3.5734570026397705\n",
      "loss: 3.054426670074463\n",
      "loss: 2.6241707801818848\n",
      "loss: 2.2674965858459473\n",
      "loss: 1.9718271493911743\n",
      "loss: 1.7267258167266846\n",
      "loss: 1.523544192314148\n",
      "loss: 1.3705692291259766\n"
     ]
    }
   ],
   "source": [
    "for i in range(STEPS):\n",
    "    # 추론\n",
    "    pred = model(inputs)\n",
    "    # 오차계산\n",
    "    loss = mse_loss_fn(pred, targets)\n",
    "    # gradient 계산\n",
    "    loss.backward()\n",
    "    # 파라미터(weights, bias) 값들 업데이트\n",
    "    optimizer.step()\n",
    "    # 파라미터의 gradient값 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 10 step당 loss 출력\n",
    "    if i % 10 == 0 or i == (STEPS-1):\n",
    "        print(f\"loss: {loss.item()}\")  # tensor객체.item() : scalar나 원소가 1개인 tensor의 값을 추출(파이썬 타입 값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습전 파라미터\n",
      "tensor([[ 1.5410, -0.2934],\n",
      "        [-2.1788,  0.5684],\n",
      "        [-1.0845, -1.3986]], requires_grad=True)\n",
      "tensor([0.4033, 0.8380], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('학습전 파라미터')\n",
    "print(weights)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습후 파라미터\n",
      "tensor([[-0.3910, -0.2828],\n",
      "        [ 0.8682,  0.8374],\n",
      "        [ 0.6338,  0.7962]], requires_grad=True)\n",
      "tensor([0.3987, 0.8516], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('학습후 파라미터')\n",
    "print(weights)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_value = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.2775,  70.5489],\n",
       "        [ 81.7814,  99.7641],\n",
       "        [119.4813, 134.6364],\n",
       "        [ 21.2970,  37.4738],\n",
       "        [101.1330, 117.4618]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch built-in 모델을 사용해 Linear Regression 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # 모델들을 제공하는 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "Pytorch는 nn.Linear 클래스를 통해 Linear Regression 모델을 제공한다.  \n",
    "nn.Linear에 입력 feature의 개수와 출력 값의 개수를 지정하면 random 값으로 초기화한 weight와 bias들을 생성해 모델을 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5555,  0.2908, -0.3628],\n",
      "        [-0.1039,  0.2114, -0.2277]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3859, -0.0908], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Linear(input featrue개수, output 개수)  모델 정의\n",
    "model = nn.Linear(3, 2)\n",
    "# weight와 bias를 조회\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer와 Loss 함수 정의\n",
    "- torch.optim 모듈에 다양한 Optimizer 클래스가 구현되있다. 그 중에서 Adam를 사용한다.\n",
    "- torch.nn 또는 torch.nn.functional 모듈에 다양한 Loss 함수가 제공된다. 이중 mse_loss() 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LOSS 함수 정의\n",
    "# or torch.nn.funcional.mse_Loss() 함수\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "# ([최적화대상-모델의 파라미터]m lr=학습률)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-36.2812,  -3.3005],\n",
       "        [-47.7916,  -5.5121],\n",
       "        [-30.0153,   5.9918],\n",
       "        [-57.1942, -10.0191],\n",
       "        [-35.4203,  -2.9023]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습전에 추론 -> 오차 계산\n",
    "# 추론\n",
    "pred = model(inputs)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12266.0420, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 오차 계산\n",
    "loss = loss_fn(pred, targets)  # (모델예측결과, 정답)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train\n",
    "주어진 epoch 만큼 학습하는 `fit`  함수를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(num_epochs:'몇번 반복할지', model:'학습시킬모델', loss_fn:'오차함수', optim:'옵티마이저', inputs:'입력데이터', targets:'출력데이터'):\n",
    "    for epoch in range(num_epochs):\n",
    "        # 1. 추론\n",
    "        pred = model(inputs)\n",
    "        # 2. 오차 계산\n",
    "        loss = loss_fn(pred, targets)\n",
    "        # 3. gradient 계산\n",
    "        loss.backward()\n",
    "        # 4. 파라미터 update\n",
    "        optim.step()\n",
    "        # 5. 계산된 gradient값 초기호\n",
    "        optim.zero_grad()\n",
    "        #### loss(결과) 출력\n",
    "        if epoch % 10 == 0 or epoch == (num_epochs-1):\n",
    "            print(f'{epoch}/{num_epochs}: train loss: {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/200: train loss: 3112.02393\n",
      "10/200: train loss: 102.43396\n",
      "20/200: train loss: 46.95527\n",
      "30/200: train loss: 34.53005\n",
      "40/200: train loss: 27.55506\n",
      "50/200: train loss: 22.61864\n",
      "60/200: train loss: 18.75453\n",
      "70/200: train loss: 15.61285\n",
      "80/200: train loss: 13.02515\n",
      "90/200: train loss: 10.88451\n",
      "100/200: train loss: 9.11120\n",
      "110/200: train loss: 7.64153\n",
      "120/200: train loss: 6.42326\n",
      "130/200: train loss: 5.41340\n",
      "140/200: train loss: 4.57625\n",
      "150/200: train loss: 3.88229\n",
      "160/200: train loss: 3.30700\n",
      "170/200: train loss: 2.83012\n",
      "180/200: train loss: 2.43479\n",
      "190/200: train loss: 2.10708\n",
      "199/200: train loss: 1.86035\n"
     ]
    }
   ],
   "source": [
    "fit(200, model, loss_fn, optimizer, inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.3179,  70.3998],\n",
       "        [ 81.4350,  99.7964],\n",
       "        [120.2043, 134.7997],\n",
       "        [ 21.4996,  37.5322],\n",
       "        [100.4109, 117.2990]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 후 모델 추론 결과 확인\n",
    "p = model(inputs)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8354194164276123"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = loss_fn(p, targets)\n",
    "l.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
